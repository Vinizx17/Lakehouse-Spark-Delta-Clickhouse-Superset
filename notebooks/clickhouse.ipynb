{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac5a560-9f6b-465f-91d7-2fa7359e9558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fato_pedidos] Último timestamp no ClickHouse: 1970-01-01 00:00:00, último ID: \n",
      "[fato_pedidos] Linhas a enviar: 25035\n",
      "[fato_pedidos] Dados inseridos com sucesso.\n",
      "[dim_cliente] Último timestamp no ClickHouse: 1970-01-01 00:00:00, último ID: \n",
      "[dim_cliente] Linhas a enviar: 1590\n",
      "[dim_cliente] Dados inseridos com sucesso.\n",
      "[dim_produto] Último timestamp no ClickHouse: 1970-01-01 00:00:00, último ID: \n",
      "[dim_produto] Linhas a enviar: 10292\n",
      "[dim_produto] Dados inseridos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_trunc\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class DeltaToClickHouse:\n",
    "    def __init__(self, minio_endpoint, access_key, secret_key, clickhouse_url, clickhouse_user, clickhouse_password):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Delta to ClickHouse\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.clickhouse:clickhouse-jdbc:0.4.6\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        self.jdbc_options = {\n",
    "            \"url\": clickhouse_url,\n",
    "            \"user\": clickhouse_user,\n",
    "            \"password\": clickhouse_password,\n",
    "            \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "        }\n",
    "\n",
    "    def get_max_ts_and_id_from_clickhouse(self, table, id_col):\n",
    "        # Busca max timestamp e max id para filtro incremental mais preciso\n",
    "        query = f\"\"\"\n",
    "            (SELECT\n",
    "                max(process_timestamp) as max_ts,\n",
    "                max({id_col}) as max_id\n",
    "            FROM {table}) AS max_table\n",
    "        \"\"\"\n",
    "        df = self.spark.read.format(\"jdbc\") \\\n",
    "            .options(**self.jdbc_options) \\\n",
    "            .option(\"dbtable\", query) \\\n",
    "            .load()\n",
    "\n",
    "        row = df.collect()[0]\n",
    "        max_ts = row['max_ts'] if row['max_ts'] is not None else datetime(1900, 1, 1)\n",
    "        max_id = row['max_id'] if row['max_id'] is not None else ''\n",
    "        return max_ts, max_id\n",
    "\n",
    "    def load_delta_and_filter(self, delta_path, max_ts, max_id, id_col):\n",
    "        df = self.spark.read.format(\"delta\").load(delta_path)\n",
    "        # Trunca timestamp para segundos para comparar no mesmo nível de granularidade\n",
    "        df = df.withColumn(\"process_timestamp_trunc\", date_trunc(\"second\", col(\"process_timestamp\")))\n",
    "\n",
    "        # Ajusta max_ts para truncar também, garantindo o filtro correto\n",
    "        from pyspark.sql.functions import lit\n",
    "        from pyspark.sql.types import TimestampType\n",
    "        max_ts_trunc = max_ts.replace(microsecond=0)\n",
    "\n",
    "        # Filtra linhas onde:\n",
    "        # process_timestamp_trunc > max_ts_trunc\n",
    "        # ou process_timestamp_trunc == max_ts_trunc e id_col > max_id\n",
    "        df_filtered = df.filter(\n",
    "            (col(\"process_timestamp_trunc\") > lit(max_ts_trunc).cast(TimestampType())) |\n",
    "            ((col(\"process_timestamp_trunc\") == lit(max_ts_trunc).cast(TimestampType())) & (col(id_col) > max_id))\n",
    "        ).drop(\"process_timestamp_trunc\")\n",
    "\n",
    "        # Opcional: deduplicar baseado no id_col para evitar duplicatas\n",
    "        df_filtered = df_filtered.dropDuplicates([id_col])\n",
    "\n",
    "        return df_filtered\n",
    "\n",
    "    def write_to_clickhouse(self, df, table):\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"dbtable\", table) \\\n",
    "            .options(**self.jdbc_options) \\\n",
    "            .save()\n",
    "\n",
    "    def sync_table(self, delta_path, table, id_col):\n",
    "        max_ts, max_id = self.get_max_ts_and_id_from_clickhouse(table, id_col)\n",
    "        print(f\"[{table}] Último timestamp no ClickHouse: {max_ts}, último ID: {max_id}\")\n",
    "\n",
    "        df_filtered = self.load_delta_and_filter(delta_path, max_ts, max_id, id_col)\n",
    "\n",
    "        print(f\"[{table}] Linhas a enviar: {df_filtered.count()}\")\n",
    "        self.write_to_clickhouse(df_filtered, table)\n",
    "        print(f\"[{table}] Dados inseridos com sucesso.\")\n",
    "\n",
    "    def run(self):\n",
    "        self.sync_table(\"s3a://staging/fato_pedidos\", \"fato_pedidos\", \"ID_Pedido\")\n",
    "        self.sync_table(\"s3a://staging/dim_cliente\", \"dim_cliente\", \"ID_Cliente\")\n",
    "        self.sync_table(\"s3a://staging/dim_produto\", \"dim_produto\", \"Product_ID\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()  # Carrega variáveis de ambiente do .env, se houver\n",
    "\n",
    "    sync = DeltaToClickHouse(\n",
    "        minio_endpoint=\"http://minio:9000\",\n",
    "        access_key=\"jGL83XVOmaZOGSKipZzb\",\n",
    "        secret_key=\"bUE3WUc0tvplgyss01XlMmpHSLsoZCkEomgCh93s\",\n",
    "        clickhouse_url=\"jdbc:clickhouse://clickhouse:8123/pedidos\",\n",
    "        clickhouse_user=os.getenv(\"CLICKHOUSE_USER\"),\n",
    "        clickhouse_password=os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
    "    )\n",
    "\n",
    "    sync.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
